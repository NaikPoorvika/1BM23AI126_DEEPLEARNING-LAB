
import numpy as np
import matplotlib.pyplot as plt

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def tanh(z):
    return np.tanh(z)

def relu(z):
    return np.maximum(0, z)

class Neuron:
    def __init__(self, weights, bias, activation="sigmoid"):
        self.weights = np.array(weights)
        self.bias = bias
        self.activation = activation
    def forward(self, inputs):
        z =  np.dot(inputs, self.weights)+self.bias
        if self.activation == "sigmoid":
            return sigmoid(z)
        elif self.activation == "tanh":
            return tanh(z)
        elif self.activation == "relu":
            return relu(z)
        else:
            raise ValueError("Unknown activation function")
            
inputs = np.array([0.5, -1.2, 3.0])
weights = [0.4, -0.6, 0.2]
bias = 0.1

for act in ["sigmoid", "tanh", "relu"]:
    neuron = Neuron(weights, bias, activation=act)
    output = neuron.forward(inputs)
    print(f"Activation = {act:7s} -->output = {output:.4f}")

z = np.linspace(-10, 10, 400)

sigmoid_vals = sigmoid(z)
tanh_vals = tanh(z)
relu_vals = relu(z)

fig, axs = plt.subplots(3, 1, figsize=(6, 12))

axs[0].plot(z, sigmoid_vals, label="Sigmoid", color='blue')
axs[0].set_title("Sigmoid Activation")
axs[0].grid(True)
axs[0].set_ylim([-0.1, 1.1])


axs[1].plot(z, tanh_vals, label="Tanh", color='orange')
axs[1].set_title("Tanh Activation")
axs[1].grid(True)
axs[1].set_ylim([-1.1, 1.1])

axs[2].plot(z, relu_vals, label="ReLU", color='green')
axs[2].set_title("ReLU Activation")
axs[2].grid(True)
axs[2].set_ylim([-1, 10])

plt.tight_layout()
plt.show()
